{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Transformer Experiment"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## src"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Configs"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "## Dataset Configurations\n",
                "TRAIN_DATASETS_PATH = './data/train_data'\n",
                "NORMALIZE_TRAIN_DATA_PATH = './data/norm_train_data'\n",
                "TEST_DATASETS_PATH = './data/test_data'\n",
                "NORMALIZE_TEST_DATA_PATH = './data/norm_test_data'\n",
                "CONTRACTIONS_PATH = './data/contractions.json'\n",
                "GENERATE_PATH = './generated'\n",
                "MODELS_PATH = './checkpoints'\n",
                "PLOTS_PATH = './plots'\n",
                "LOGS_PATH = './logs'\n",
                "\n",
                "## Training Configurations\n",
                "BATCH_SIZE = 128\n",
                "BLOCK_SIZE = 100\n",
                "LSTM_CONFIGS = dict(\n",
                "    embedding_dim=64,\n",
                "    hidden_dim=64,\n",
                "    num_layers=2,\n",
                "    dropout=0.4\n",
                ")\n",
                "TRANSFORMER_CONFIGS = dict(\n",
                "    embedding_dim=64,\n",
                "    n_head=2,\n",
                "    n_encoders=2,\n",
                "    n_decoders=2,\n",
                "    dim_feedforward=64,\n",
                "    dropout=0.2\n",
                ")\n",
                "LEARNING_RATE=1e-3\n",
                "WEIGHT_DECAY=0.01\n",
                "EPOCHS=40\n",
                "GAMMA=0.98\n",
                "NUM_TRAIN_DATA=4\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Utils"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import re\n",
                "import json\n",
                "from typing import Dict\n",
                "\n",
                "\n",
                "\n",
                "def load_contractions(file_path: str) -> Dict[str, str]:\n",
                "    '''\n",
                "    Load contractions from a JSON file.\n",
                "\n",
                "    Args:\n",
                "        file_path (str): The path to the JSON file containing contractions.\n",
                "\n",
                "    Returns:\n",
                "        Dict[str, str]: A dictionary mapping contractions to their expanded forms.\n",
                "    '''\n",
                "    with open(file_path, 'r') as f:\n",
                "        return json.load(f)\n",
                "\n",
                "\n",
                "def expand_contractions(text: str, contractions_dict: Dict[str, str]) -> str:\n",
                "    '''\n",
                "    Expand contractions in a given text using a provided contractions dictionary.\n",
                "\n",
                "    Args:\n",
                "        text (str): The input text containing contractions.\n",
                "        contractions_dict (Dict[str, str]): A dictionary mapping contractions to their expanded forms.\n",
                "\n",
                "    Returns:\n",
                "        str: The text with contractions expanded.\n",
                "    '''\n",
                "    # Compile the regular expression pattern for matching contractions\n",
                "    contractions_pattern = re.compile(\n",
                "        '|'.join(re.escape(key) for key in contractions_dict.keys()),\n",
                "        flags=re.IGNORECASE\n",
                "    )\n",
                "\n",
                "    # Function to replace each contraction with its expanded form\n",
                "    def replace(match):\n",
                "        return contractions_dict[match.group(0).lower()]\n",
                "\n",
                "    # Substitute contractions in the text using the replace function\n",
                "    return contractions_pattern.sub(replace, text)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "\n",
                "\n",
                "def get_device() -> torch.device:\n",
                "    '''\n",
                "    Return CUDA device if cuda is available\n",
                "    '''\n",
                "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "\n",
                "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
                "\n",
                "\n",
                "def accuracy_fn(model_logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
                "    '''\n",
                "    Compute the accuracy of a classification model.\n",
                "\n",
                "    Args:\n",
                "      \tmodel_logits (torch.Tensor): The logits or outputs from the model, of shape (N, C),\n",
                "\t\t\twhere N is the number of samples and C is the number of classes.\n",
                "    \tlabels (torch.Tensor): The true labels, of shape (N,), where each value is in the range [0, C-1].\n",
                "\n",
                "    Returns:\n",
                "    \tfloat: The accuracy of the model on the provided batch of data, in percentage (%).\n",
                "    '''\n",
                "    preds = torch.softmax(model_logits, dim=1).argmax(dim=1)\n",
                "\n",
                "    return (torch.sum(preds == labels).item() / len(labels))\n",
                "\n",
                "def get_perplexity(model_logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
                "    '''\n",
                "    Calculate the perplexity of a language model.\n",
                "\n",
                "    Args:\n",
                "    \tmodel_logits (torch.Tensor): The logits or outputs from the model.\n",
                "    \tlabels (torch.Tensor): The true labels.\n",
                "\n",
                "    Returns:\n",
                "    \tfloat: The perplexity of the model.\n",
                "    '''\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "\n",
                "    loss = criterion(model_logits, labels.to(torch.long))\n",
                "\n",
                "    perplexity = torch.exp(loss)\n",
                "\n",
                "    return perplexity.item()\n",
                "\n",
                "def get_precision(model_logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
                "    '''\n",
                "    Compute the precision of a classification model.\n",
                "\n",
                "    Args:\n",
                "    \tmodel_logits (torch.Tensor): The logits or outputs from the model.\n",
                "    \tlabels (torch.Tensor): The true labels.\n",
                "\n",
                "    Returns:\n",
                "    \tfloat: The precision of the model.\n",
                "    '''\n",
                "    preds = torch.softmax(model_logits, dim=1).argmax(dim=1)\n",
                "\n",
                "    preds = preds.detach().cpu().numpy()\n",
                "    labels = labels.detach().cpu().numpy().astype('int64')\n",
                "\n",
                "    return precision_score(labels, preds, average='weighted')\n",
                "\n",
                "def get_recall(model_logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
                "    '''\n",
                "    Compute the recall of a classification model.\n",
                "\n",
                "    Args:\n",
                "    \tmodel_logits (torch.Tensor): The logits or outputs from the model.\n",
                "    \tlabels (torch.Tensor): The true labels.\n",
                "\n",
                "    Returns:\n",
                "\t\tfloat: The recall of the model.\n",
                "    '''\n",
                "    preds = torch.softmax(model_logits, dim=1).argmax(dim=1)\n",
                "\n",
                "    preds = preds.detach().cpu().numpy()\n",
                "    labels = labels.detach().cpu().numpy().astype('int64')\n",
                "\n",
                "    return recall_score(labels, preds, average='weighted')\n",
                "\n",
                "def get_f1_score(model_logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
                "    '''\n",
                "    Compute the F1 score of a classification model.\n",
                "\n",
                "    Args:\n",
                "    \tmodel_logits (torch.Tensor): The logits or outputs from the model.\n",
                "    \tlabels (torch.Tensor): The true labels.\n",
                "\n",
                "    Returns:\n",
                "    \tfloat: The F1 score of the model.\n",
                "    '''\n",
                "    preds = torch.softmax(model_logits, dim=1).argmax(dim=1)\n",
                "\n",
                "    preds = preds.detach().cpu().numpy()\n",
                "    labels = labels.detach().cpu().numpy().astype('int64')\n",
                "\n",
                "    return f1_score(labels, preds, average='weighted')\n",
                "\n",
                "def get_specificity(model_logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
                "    '''\n",
                "    Compute the specificity of a classification model.\n",
                "\n",
                "    Args:\n",
                "    \tmodel_logits (torch.Tensor): The logits or outputs from the model.\n",
                "    \tlabels (torch.Tensor): The true labels.\n",
                "\n",
                "    Returns:\n",
                "    \tfloat: The specificity of the model.\n",
                "    '''\n",
                "    preds = torch.softmax(model_logits, dim=1).argmax(dim=1)\n",
                "\n",
                "    preds = preds.detach().cpu().numpy()\n",
                "    labels = labels.detach().cpu().numpy()\n",
                "\n",
                "    # Get the confusion matrix\n",
                "    cm = confusion_matrix(labels, preds)\n",
                "\n",
                "    # Calculate specificity for each class\n",
                "    average_specificity = 0\n",
                "    for i in range(len(cm)):\n",
                "        # True Positives, False Positives, False Negatives, and True Negatives\n",
                "        TP = cm[i, i]\n",
                "        FP = cm[:, i].sum() - TP\n",
                "        FN = cm[i, :].sum() - TP\n",
                "        TN = cm.sum() - (TP + FP + FN)\n",
                "        \n",
                "        # Calculate specificity\n",
                "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
                "\n",
                "        average_specificity += specificity\n",
                "    \n",
                "    average_specificity /= len(cm)\n",
                "\n",
                "    return average_specificity\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import logging\n",
                "\n",
                "\n",
                "def configure_logger(name=__name__, log_file='app.log', level=logging.INFO) -> logging.Logger:\n",
                "    '''\n",
                "    Configures the logger with console and file handlers.\n",
                "\n",
                "    Args:\n",
                "        name (str): The name of the logger.\n",
                "        log_file (str): The file to log messages to.\n",
                "        level (int): The logging level.\n",
                "    \n",
                "    Return:\n",
                "        The logger object.\n",
                "    '''\n",
                "    logger = logging.getLogger(name)\n",
                "    logger.setLevel(level)\n",
                "    logger.propagate = False # do not pass logs to the default logger\n",
                "\n",
                "    # Check if the logger already has handlers to prevent adding multiple handlers\n",
                "    if not logger.handlers:\n",
                "        # Create the formatter object for the logger\n",
                "        file_formatter = logging.Formatter('%(asctime)s \\t %(filename)s \\t %(levelname)s \\t %(message)s')\n",
                "        stdout_formatter = logging.Formatter('%(levelname)s \\t %(message)s')\n",
                "\n",
                "        # Create the console handler and setting its level\n",
                "        console_handler = logging.StreamHandler()\n",
                "        console_handler.setLevel(level)\n",
                "\n",
                "        # Create the file handler and setting its level\n",
                "        file_handler = logging.FileHandler(log_file)\n",
                "        file_handler.setLevel(level)\n",
                "\n",
                "        # Add the formatter to the handlers\n",
                "        console_handler.setFormatter(stdout_formatter)\n",
                "        file_handler.setFormatter(file_formatter)\n",
                "\n",
                "        # Add the handlers to the logger\n",
                "        logger.addHandler(console_handler)\n",
                "        logger.addHandler(file_handler)\n",
                "    \n",
                "    return logger\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "\n",
                "\n",
                "def temperature_sampling(model_logits: torch.Tensor, temperature:float=1.0) -> int:\n",
                "    '''\n",
                "    Perform temperature sampling to generate the next token based on model logits.\n",
                "\n",
                "    Args:\n",
                "    - model_logits (torch.Tensor): Logits (raw predictions) from the model, typically for the next\n",
                "        token prediction. Should have shape [sequence_length, vocab_size].\n",
                "    - temperature (float): Temperature parameter to scale the logits before applying softmax.\n",
                "        Higher values make the probability distribution flatter (more random), while lower values\n",
                "        make it sharper (more deterministic). Default is 1.0.\n",
                "\n",
                "    Returns:\n",
                "    - int: The sampled token index from the probability distribution.\n",
                "    '''\n",
                "    # Get the logits for the last character in the sequence\n",
                "    # logits = model_logits[-1, :]\n",
                "    # Scale them using temperature to affect the steepness of the underling distrubution\n",
                "    # logits = logits / temperature\n",
                "    logits = model_logits / temperature\n",
                "    # Generate probabilities\n",
                "    probs = torch.softmax(logits, dim=-1)\n",
                "    # Sample from the probability distribution\n",
                "    next_token = torch.multinomial(probs, num_samples=1).item()\n",
                "\n",
                "    return next_token\n",
                "\n",
                "def create_tgt(prev_batch: torch.Tensor, x_batch: torch.Tensor) -> torch.Tensor:\n",
                "    '''\n",
                "    Generate a target tensor for sequence models by shifting the input sequence to the\n",
                "    right and prepending a previous batch tensor.\n",
                "\n",
                "    Args:\n",
                "        prev_batch (torch.Tensor): A tensor containing the previous batch data, typically the start-of-sequence tokens.\n",
                "        x_batch (torch.Tensor): A tensor containing the current batch of input sequences.\n",
                "\n",
                "    Returns:\n",
                "        torch.Tensor: A tensor where the `x_batch` is shifted to the right, and the `prev_batch` is prepended as the first element in the sequence.\n",
                "    '''\n",
                "    # Initialize tgt with zeros (assuming 0 is the start-of-sequence token)\n",
                "    tgt = torch.zeros_like(x_batch)\n",
                "\n",
                "    # Shift x_batch to the right and append y_batch as the last element\n",
                "    tgt[:, 0] = prev_batch\n",
                "    tgt[:, 1:] = x_batch[:, :-1]\n",
                "\n",
                "    return tgt\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src.utils.log import configure_logger\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "from pathlib import Path\n",
                "from os import remove\n",
                "\n",
                "\n",
                "# Get the logger for this module\n",
                "logger = configure_logger(__name__)\n",
                "\n",
                "\n",
                "def save_model(model: nn.Module, path: str, stops=False) -> None:\n",
                "    '''\n",
                "    Save a PyTorch model to a specified path.\n",
                "\n",
                "    Args:\n",
                "        model (torch.Module): The PyTorch model to be saved.\n",
                "        path (str): The path where the model will be saved.\n",
                "        stops (bool, optional): If True, stops the function execution if the model file already exists at the given path. Defaults to False.\n",
                "\n",
                "    Raises:\n",
                "        AssertionError: If the file extension of the specified path is not `.pt` or `.pth`.\n",
                "\n",
                "    Returns:\n",
                "        None\n",
                "    '''\n",
                "    target_path = Path('/'.join(path.split('/')[:-1]))\n",
                "    model_name = path.split('/')[-1]\n",
                "\n",
                "    if not (model_name.endswith('.pth') or model_name.endswith('.pt')):\n",
                "        logger.error('Wrong extension: Expecting `.pt` or `.pth`.')\n",
                "        return\n",
                "    \n",
                "    # Creating the directory that the model is going to be saved if not exists\n",
                "    if not target_path.exists():\n",
                "        target_path.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    # If path already exists\n",
                "    if Path(path).is_file():\n",
                "        logger.info(f'Model `{model_name}` already exists on `{target_path}`.')\n",
                "        if stops:\n",
                "            return\n",
                "        logger.info(f'Deleting `{path}`.')\n",
                "        remove(path)\n",
                "\n",
                "    # Saving the Model to the given path\n",
                "    logger.info(f'Saving Model `{model_name}` to `{target_path}`.')\n",
                "    torch.save(model.state_dict(), path)\n",
                "\n",
                "    logger.info(f'Model Successfully Saved to `{path}`.')\n",
                "\n",
                "\n",
                "def load_model(model_class: nn.Module, model_path: str, device: torch.device=torch.device('cpu'), model_device: bool=False, **kwargs) -> nn.Module:\n",
                "    '''\n",
                "    Loads a PyTorch model from a specified file.\n",
                "    \n",
                "    Parameters:\n",
                "        model_path (str): Path to the saved model file (e.g., 'model.pth').\n",
                "        model_class (nn.Module): The class of the model to be loaded.\n",
                "        device (torch.device): The device that the model will be load on. Default is CPU.\n",
                "        model_device (bool): If True the model needs device in its arguments. Default is False\n",
                "        **kwargs: Additional arguments required to initialize the model class.\n",
                "\n",
                "    Returns:\n",
                "        The loaded model.\n",
                "    '''\n",
                "    # Initialize the model\n",
                "    if model_device:\n",
                "        model = model_class(device=device, **kwargs)\n",
                "    else:\n",
                "        model = model_class(**kwargs)\n",
                "\n",
                "    # Load the state dict (parameters)\n",
                "    state_dict = torch.load(model_path, map_location=torch.device(device))\n",
                "    \n",
                "    # Load the parameters into the model\n",
                "    model.load_state_dict(state_dict)\n",
                "    \n",
                "    # Set the model to evaluation mode\n",
                "    model.eval()\n",
                "\n",
                "    logger.info('Model succesfully loaded.')\n",
                "    \n",
                "    return model\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import json\n",
                "\n",
                "import os\n",
                "from typing import List, Union, Dict, Any\n",
                "\n",
                "\n",
                "def __load_notebook(notebook_path: str) -> Dict[str, Any]:\n",
                "    '''\n",
                "    Loads an existing Jupyter notebook.\n",
                "    \n",
                "    Args:\n",
                "        notebook_path (str): Path to the .ipynb file to load.\n",
                "        \n",
                "    Returns:\n",
                "        dict: The notebook content as a dictionary.\n",
                "    '''\n",
                "    if os.path.exists(notebook_path):\n",
                "        with open(notebook_path, 'r') as f:\n",
                "            notebook = json.load(f)\n",
                "    else:\n",
                "        # If the file doesn't exist, create an empty notebook structure\n",
                "        notebook = {\n",
                "            'cells': [],\n",
                "            'metadata': {\n",
                "                'kernelspec': {\n",
                "                    'display_name': 'Python 3',\n",
                "                    'language': 'python',\n",
                "                    'name': 'python3'\n",
                "                },\n",
                "                'language_info': {\n",
                "                    'codemirror_mode': {\n",
                "                        'name': 'ipython',\n",
                "                        'version': 3\n",
                "                    },\n",
                "                    'file_extension': '.py',\n",
                "                    'mimetype': 'text/x-python',\n",
                "                    'name': 'python',\n",
                "                    'nbconvert_exporter': 'python',\n",
                "                    'pygments_lexer': 'ipython3',\n",
                "                    'version': '3.x'\n",
                "                }\n",
                "            },\n",
                "            'nbformat': 4,\n",
                "            'nbformat_minor': 2\n",
                "        }\n",
                "    return notebook\n",
                "\n",
                "\n",
                "def py_to_ipynb(\n",
                "        py_files: List[str],\n",
                "        output_ipynb: str,\n",
                "        comment: Union[str, None]=None\n",
                "    ) -> None:\n",
                "    '''\n",
                "    Converts a list of Python (.py) files into a Jupyter Notebook (.ipynb) with optional comments.\n",
                "    \n",
                "    Args:\n",
                "        py_files (list of str): List of paths to the .py files.\n",
                "        output_ipynb (str): Path to the output .ipynb file.\n",
                "        comment (list of str): The comment markdown of the notebook.\n",
                "    '''\n",
                "    # Load the existing notebook or create a new one\n",
                "    notebook = __load_notebook(output_ipynb)\n",
                "\n",
                "    # Add a Markdown cell for the comment if provided\n",
                "    if comment:\n",
                "        markdown_cell = {\n",
                "            'cell_type': 'markdown',\n",
                "            'metadata': {},\n",
                "            'source': comment\n",
                "        }\n",
                "        notebook['cells'].append(markdown_cell)  # Append the markdown cell to the notebook\n",
                "\n",
                "    # Iterate through each .py file and corresponding comment\n",
                "    for py_file in py_files:\n",
                "        if py_file.endswith('__.py') or not py_file.endswith('.py'):\n",
                "            continue\n",
                "\n",
                "        # Read the Python (.py) file content\n",
                "        with open(py_file, 'r') as f:\n",
                "            source_code = f.read()\n",
                "        \n",
                "        # Create a code cell for the Python script content\n",
                "        code_cell = {\n",
                "            'cell_type': 'code',\n",
                "            'metadata': {},\n",
                "            'source': source_code.splitlines(True),  # Split lines to maintain formatting\n",
                "            'outputs': [],\n",
                "            'execution_count': None\n",
                "        }\n",
                "        \n",
                "        # Append the code cell to the list of cells\n",
                "        notebook['cells'].append(code_cell)\n",
                "    \n",
                "    \n",
                "    # Write the notebook to a file\n",
                "    with open(output_ipynb, 'w') as f:\n",
                "        json.dump(notebook, f, indent=4)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import DataLoader, Dataset, Subset\n",
                "\n",
                "import numpy as np\n",
                "import math\n",
                "\n",
                "from multiprocessing import cpu_count\n",
                "from typing import Tuple\n",
                "\n",
                "\n",
                "def get_loaders(\n",
                "        dataset: Dataset,\n",
                "        batch_size: int,\n",
                "        train_pro: float,\n",
                "        drop_last: bool=False,\n",
                "        offset: int = 0,\n",
                "        step: int = 1\n",
                "    ) -> Tuple[DataLoader, DataLoader, int]:\n",
                "    '''\n",
                "    Splits the dataset into training and validation sets based on the given proportion,\n",
                "    creates DataLoaders for each, and returns the DataLoaders along with an updated offset \n",
                "    for cyclic iteration through splits.\n",
                "\n",
                "    Args:\n",
                "        dataset (Dataset): The dataset to be split and loaded.\n",
                "        batch_size (int): Number of samples per batch.\n",
                "        train_pro (float): Proportion of data to use for training (between 0 and 1).\n",
                "        drop_last (bool, optional): Whether to drop the last incomplete batch if the dataset \n",
                "                                    size is not divisible by batch size. Default is False.\n",
                "        offset (int, optional): Starting point for the train/validation split to allow for \n",
                "                                cyclic shifting of the split. Default is 0.\n",
                "        step (int, optional): Amount to increment the offset after each function call, \n",
                "                              useful for iterating through different splits. Default is 1.\n",
                "\n",
                "    Returns:\n",
                "        Tuple[DataLoader, DataLoader, int]:\n",
                "            - `train_loader`: DataLoader for the training set.\n",
                "            - `valid_loader`: DataLoader for the validation set.\n",
                "            - `offset`: The updated offset value for the next split.\n",
                "    '''\n",
                "    # Use NumPy for efficient index handling\n",
                "    indices = np.arange(len(dataset))\n",
                "    # num_splits = len(dataset) // batch_size + (1 if len(dataset) % batch_size != 0 else 0)\n",
                "    num_splits = len(dataset) // batch_size\n",
                "    splits = np.array_split(indices, num_splits)\n",
                "\n",
                "    train_splits_size = int(len(splits) * train_pro)\n",
                "    valid_splits_size = len(splits) - train_splits_size\n",
                "\n",
                "    # Create train splits with wrapping around if necessary\n",
                "    if train_splits_size + offset > len(splits):\n",
                "        train_splits = splits[offset:] + splits[: (train_splits_size + offset) % len(splits)]\n",
                "    else:\n",
                "        train_splits = splits[offset: train_splits_size + offset]\n",
                "\n",
                "    # Create valid splits with wrapping around if necessary\n",
                "    valid_offset = (train_splits_size + offset) % len(splits)\n",
                "    if valid_offset + valid_splits_size > len(splits):\n",
                "        valid_splits = splits[valid_offset:] + splits[: (valid_offset + valid_splits_size) % len(splits)]\n",
                "    else:\n",
                "        valid_splits = splits[valid_offset: valid_offset + valid_splits_size]\n",
                "\n",
                "    # Flatten the lists of batches to lists of indices\n",
                "    train_indices = np.concatenate(train_splits)\n",
                "    valid_indices = np.concatenate(valid_splits)\n",
                "\n",
                "    # Creating DataLoaders\n",
                "    train_loader = DataLoader(Subset(dataset, train_indices), batch_size=batch_size, shuffle=True, drop_last=drop_last, num_workers=cpu_count(), pin_memory=True)\n",
                "    valid_loader = DataLoader(Subset(dataset, valid_indices), batch_size=batch_size, shuffle=False, drop_last=drop_last, num_workers=cpu_count(), pin_memory=True)\n",
                "\n",
                "    # Update the offset\n",
                "    offset = (offset + step) % len(splits)\n",
                "\n",
                "    return train_loader, valid_loader, offset\n",
                "\n",
                "\n",
                "class PositionalEncoding(nn.Module):\n",
                "    def __init__(self, block_size: int, d_model: int) -> None:\n",
                "        super().__init__()\n",
                "\n",
                "        # Positional encoding matrix\n",
                "        pe = torch.zeros(block_size, d_model)\n",
                "\n",
                "        # Position indices\n",
                "        position = torch.arange(0, block_size, dtype=torch.float).unsqueeze(1)\n",
                "\n",
                "        # Scaling factors for positions\n",
                "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
                "\n",
                "        # Sine and cosine positional encodings\n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "\n",
                "        pe = pe.unsqueeze(0)\n",
                "        self.register_buffer('pe', pe) # buffers are tensors that are not updated during training\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        return self.pe[:, :x.size(1), :]\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src.utils.log import configure_logger\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import os\n",
                "from typing import List, Tuple, Optional\n",
                "\n",
                "\n",
                "# Get the logger for this module\n",
                "logger = configure_logger(__name__)\n",
                "\n",
                "\n",
                "def plot_loss(\n",
                "        loss_list: List[float],\n",
                "        type: str = 'eval',\n",
                "        c: str = 'g',\n",
                "        figsize: Tuple[int, int] = (6, 4),\n",
                "        fontsize: int = 14,\n",
                "        save_path: Optional[str] = None\n",
                "    ) -> None:\n",
                "    '''\n",
                "    Plots the training or evaluation loss over epochs and optionally saves the plot to a specified path.\n",
                "\n",
                "    Args:\n",
                "        loss_list (List[float]): List of loss values to be plotted.\n",
                "        type (str, optional): Type of loss. Default is 'eval'. Accepted values are 'train' or 'eval'.\n",
                "        c (str, optional): Color of the plot. Default is 'g' (green).\n",
                "        figsize (Tuple, optional): Size of the figure (width, height) in inches. Default is (6, 4).\n",
                "        fontsize (int, optional): Font size of the title. Default is 14.\n",
                "        save_path (Optional[str], optional): Path to save the plot image. If None, the plot is not saved. Default is None.\n",
                "\n",
                "    Raises:\n",
                "        AssertionError: If an invalid loss type is provided.\n",
                "\n",
                "    Returns:\n",
                "        None\n",
                "    '''\n",
                "\n",
                "    if type not in ['train', 'eval']:\n",
                "        logger.error(f'Invalid loss type: Got `{type}`. Only `train`, `eval` are accepted')\n",
                "        return\n",
                "\n",
                "    plt.figure(figsize=figsize)\n",
                "\n",
                "    if type == 'eval':\n",
                "        plt.plot(range(len(loss_list)), loss_list, c=c, label=\"Validation Lost\")\n",
                "        plt.title(f\"Validation Loss\", fontsize=fontsize)\n",
                "    else:\n",
                "        plt.plot(range(len(loss_list)), loss_list, c=c, label=\"Training Lost\")\n",
                "        plt.title(f\"Training Loss\", fontsize=fontsize)\n",
                "\n",
                "    plt.xlabel(\"Epochs\")\n",
                "    plt.ylabel(\"Loss\")\n",
                "\n",
                "    if save_path:\n",
                "        # Create the directory if it doesn't exist\n",
                "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
                "        plt.savefig(save_path)\n",
                "        logger.info(f\"Plot saved to `{save_path}`.\")\n",
                "\n",
                "    plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
                "\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def plot_losses(\n",
                "        train_loss: List[float],\n",
                "        eval_loss: List[float],\n",
                "        c: List[str] = ['g', 'b'],\n",
                "        fig_size: Tuple[int, int] = (6, 4),\n",
                "        font_size: int = 11,\n",
                "        save_path: Optional[str] = None\n",
                "    ) -> None:\n",
                "    '''\n",
                "    Plots both training and evaluation losses over epochs and optionally saves the plot to a specified path.\n",
                "\n",
                "    Args:\n",
                "        train_loss (List[float]): List of training loss values to be plotted.\n",
                "        eval_loss (List[float]): List of evaluation loss values to be plotted.\n",
                "        c (List[str], optional): List of colors for the plots. Default is ['g', 'b'] (green for validation loss, blue for training loss).\n",
                "        fig_size (Tuple[int, int], optional): Size of the figure (width, height) in inches. Default is (6, 4).\n",
                "        font_size (int, optional): Font size of the legend and title. Default is 11.\n",
                "        save_path (Optional[str], optional): Path to save the plot image. If None, the plot is not saved. Default is None.\n",
                "\n",
                "    Returns:\n",
                "        None\n",
                "    '''\n",
                "\n",
                "    plt.figure(figsize=fig_size)\n",
                "\n",
                "    plt.plot(range(len(train_loss)), train_loss, c=c[0], label=\"Training Loss\")\n",
                "    plt.plot(range(len(eval_loss)), eval_loss, c=c[1], label=\"Validation Loss\")\n",
                "    plt.xlabel(\"Epochs\")\n",
                "    plt.ylabel(\"Loss\")\n",
                "    plt.title(f\"Loss Curves\", fontsize=14)\n",
                "    plt.legend(fontsize=font_size)\n",
                "\n",
                "    if save_path:\n",
                "        # Create the directory if it doesn't exist\n",
                "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
                "        plt.savefig(save_path)\n",
                "        logger.info(f\"Plot saved to `{save_path}`.\")\n",
                "\n",
                "    plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
                "\n",
                "    plt.show()\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Dataset Class"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src.utils import configure_logger\n",
                "from src.utils.data import load_contractions, expand_contractions\n",
                "\n",
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "\n",
                "import os\n",
                "import re\n",
                "import string\n",
                "from tqdm import tqdm\n",
                "from typing import Dict, List, Tuple, Union, Iterable\n",
                "\n",
                "\n",
                "class TransformerShakespeareDataset(Dataset):\n",
                "    '''\n",
                "    Dataset class for text generation using Shakespeare's works.\n",
                "    '''\n",
                "\n",
                "    # Get the logger as a class attribute\n",
                "    logger = configure_logger(__name__)\n",
                "\n",
                "    def __init__(self,\n",
                "            dataset_path: str,\n",
                "            norm_dataset_path: str,\n",
                "            constractions_path: str,\n",
                "            block_size: int,\n",
                "            to_tensors: bool=False,\n",
                "            write_norm: bool=True,\n",
                "            device: torch.device = torch.device('cpu')\n",
                "        ) -> None:\n",
                "        '''\n",
                "        Initializes a TransformerShakespeareDataset object.\n",
                "\n",
                "        Args:\n",
                "            dataset_path (str): Path to the directory containing the raw dataset files.\n",
                "            norm_dataset_path (str): Path to save the normalized dataset files.\n",
                "            contractions_path (str): Path to the file containing contractions for expansion.\n",
                "            block_size (int): Size of the sequence block for creating training samples.\n",
                "            to_tensors (bool, optional): If True, returns samples as PyTorch tensors (default: False).\n",
                "            write_norm (bool, optional): If True, writes normlized files to memory (default: True).\n",
                "            device (torch.device, optional): Device to store tensors on (default: 'cpu').\n",
                "        '''\n",
                "        super().__init__()\n",
                "\n",
                "        os.makedirs(dataset_path, exist_ok=True)\n",
                "        os.makedirs(norm_dataset_path, exist_ok=True)\n",
                "\n",
                "        self.dataset_path = dataset_path\n",
                "        self.norm_dataset_path = norm_dataset_path\n",
                "        self.constractions_path = constractions_path\n",
                "        self.block_size = block_size\n",
                "        self.to_tensors = to_tensors\n",
                "        self.device = device\n",
                "\n",
                "        if write_norm:\n",
                "            self._normalize_data()\n",
                "\n",
                "        self.vocab = self._create_vocab()\n",
                "        self.vocab_size = len(self.vocab)\n",
                "\n",
                "        # Create mapping from characters to integers\n",
                "        self.char_to_int = {char: i for i, char in enumerate(self.vocab)}\n",
                "        self.int_to_char = {i: char for i, char in enumerate(self.vocab)}\n",
                "\n",
                "        self.samples = self._create_samples()\n",
                "    \n",
                "    def __getitem__(self, index: slice) -> List[Tuple[int, Union[torch.Tensor, Iterable], int]]:\n",
                "        '''\n",
                "        Retrieves a sample from the dataset by index or slice.\n",
                "\n",
                "        Args:\n",
                "            index (int or slice): Index or slice to retrieve from the dataset.\n",
                "\n",
                "        Returns:\n",
                "            List[Tuple[Union[torch.Tensor, Iterable], int]]: List of samples, each sample is a tuple of \n",
                "                (input_sequence, target_char). Input_sequence can be a torch.Tensor if to_tensors is True, \n",
                "                otherwise an iterable (list of integers).\n",
                "        '''\n",
                "        if isinstance(index, slice):\n",
                "            if self.to_tensors:\n",
                "                return [(sample[0], torch.tensor(sample[1], dtype=torch.int64), sample[2]) for sample in self.samples[index]]\n",
                "            return [(sample[0], sample[1], sample[2]) for sample in self.samples[index]]\n",
                "        else:\n",
                "            if self.to_tensors:\n",
                "                return (self.samples[index][0], torch.tensor(self.samples[index][1], dtype=torch.int64), self.samples[index][2])\n",
                "            return (self.samples[index][0], self.samples[index][1], self.samples[index][2])\n",
                "        \n",
                "    def __len__(self) -> int:\n",
                "        '''\n",
                "        Returns the total number of samples in the dataset.\n",
                "\n",
                "        Returns:\n",
                "            int: Total number of samples in the dataset.\n",
                "        '''\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __str__(self) -> str:\n",
                "        '''\n",
                "        Returns a string representation of the dataset object.\n",
                "\n",
                "        Returns:\n",
                "            str: String representation of the dataset object.\n",
                "        '''\n",
                "        return f'TransformerShakespeareDataset(vocab_size: {self.vocab_size}, block_size: {self.block_size}, to_tensors: {self.to_tensors}, device: {self.device})'\n",
                "    \n",
                "    def _encode(self, s: str) -> List[int]:\n",
                "        '''\n",
                "        Encode a string into a list of integers using the vocabulary.\n",
                "        '''\n",
                "        return [self.char_to_int[c] for c in s]\n",
                "\n",
                "    def _decode(self, l: List[int]) -> str:\n",
                "        '''\n",
                "        Decode a list of integers into a string using the vocabulary.\n",
                "        '''\n",
                "        return ''.join([self.int_to_char[i] for i in l])\n",
                "\n",
                "    @staticmethod\n",
                "    def _create_vocab() -> List[str]:\n",
                "        '''\n",
                "        Geerate the vocabulary of the dataset, containing all lowercase english letters, all\n",
                "            digits (0-9) and the white space characters (' ', '\\n', '\\t'). It doen't contain\n",
                "            any punctuations for now.\n",
                "\n",
                "        Returns:\n",
                "            List[str]: The vocabulary as a list of strings.\n",
                "        '''\n",
                "        vocab = list(string.ascii_lowercase)\n",
                "        vocab += list(string.digits)\n",
                "        vocab += list(string.punctuation)\n",
                "        vocab += [' ', '\\n', '\\t']\n",
                "        vocab += '\\0' # <sos> token (goes at index 0)\n",
                "\n",
                "        return sorted(set(vocab))\n",
                "    \n",
                "    @staticmethod\n",
                "    def _normalize(content: str, contractions_dict: Dict[str, str]) -> str:\n",
                "        '''\n",
                "        Normilize the given text.\n",
                "\n",
                "        Args:\n",
                "            content (str): The string to be normilized\n",
                "            contractions_dict (Dict[str, str]): Dictionary of contractions for expansion.\n",
                "\n",
                "        Returns:\n",
                "            str: The normilized string.\n",
                "        '''\n",
                "        content = content.lower()\n",
                "        content = re.sub(r' +', ' ', content) # Replace multiple spaces\n",
                "        content = expand_contractions(content, contractions_dict=contractions_dict)\n",
                "\n",
                "        return content\n",
                "\n",
                "    def _proc_file(self, file: str, contractions_dict: Dict[str, str]) -> str:\n",
                "        '''\n",
                "        Processes a file by normalizing its content.\n",
                "\n",
                "        Args:\n",
                "            file (str): File to be processed.\n",
                "            contractions_dict (Dict[str, str]): Dictionary of contractions for expansion.\n",
                "\n",
                "        Returns:\n",
                "            str: Normalized content of the file.\n",
                "        '''\n",
                "        # Get the content of the file\n",
                "        file_path = os.path.join(self.dataset_path, file)\n",
                "\n",
                "        # Only identifying .txt files as datasets\n",
                "        if not file_path.endswith('.txt'):\n",
                "            return ''\n",
                "\n",
                "        with open(os.path.join(self.dataset_path, file)) as f:\n",
                "            content = f.read()\n",
                "\n",
                "        content = TransformerShakespeareDataset._normalize(content, contractions_dict)\n",
                "\n",
                "        return content\n",
                "\n",
                "    def _write_norm_file(self, text: str, file_name: str) -> None:\n",
                "        '''\n",
                "        Writes normalized text to a file.\n",
                "\n",
                "        Args:\n",
                "            text (str): Normalized text to be written.\n",
                "            file_name (str): Name of the file to write the text to.\n",
                "        '''\n",
                "        with open(os.path.join(self.norm_dataset_path, file_name), 'w') as f:\n",
                "            f.write(text)\n",
                "\n",
                "    def _normalize_data(self) -> None:\n",
                "        '''\n",
                "        Normilize the dataset files.\n",
                "        '''\n",
                "        contractions_dict = load_contractions(self.constractions_path)\n",
                "\n",
                "        for file_name in os.listdir(self.dataset_path):\n",
                "            TransformerShakespeareDataset.logger.info(f'Processing dataset: {os.path.join(self.dataset_path, file_name)}.')\n",
                "            # Normalize the content of the file\n",
                "            norm_text = self._proc_file(file_name, contractions_dict)\n",
                "\n",
                "            if norm_text == '':\n",
                "                TransformerShakespeareDataset.logger.info(f'File {file_name} is not a .txt file, so it\\'s been ignored.')\n",
                "                continue\n",
                "\n",
                "            self._write_norm_file(norm_text, file_name=file_name)\n",
                "            TransformerShakespeareDataset.logger.info(f'Normalized dataset succesfully saved to: {os.path.join(self.norm_dataset_path, file_name)}')\n",
                "\n",
                "    def _load_texts(self) -> str:\n",
                "        '''\n",
                "        Load and concatenate the text content of all files in the normalized data path.\n",
                "\n",
                "        Returns:\n",
                "            str: A single string containing the concatenated text content of all files.\n",
                "        '''\n",
                "        texts = ''\n",
                "        for file_name in os.listdir(self.norm_dataset_path):\n",
                "            with open(os.path.join(self.norm_dataset_path, file_name), 'r') as file:\n",
                "                texts += file.read()\n",
                "\n",
                "        return texts\n",
                "\n",
                "    def _create_samples(self) -> List[Tuple[int, List[int], int]]:\n",
                "        '''\n",
                "        Creates training samples from the concatenated text data.\n",
                "\n",
                "        Returns:\n",
                "            List[int, Tuple[List[int], int]]: List of training samples, where each sample is a tuple of \n",
                "                (input_sequence, target_char). Each input_sequence is a list of integers (encoded characters),\n",
                "                and target_char is the next character in the sequence.\n",
                "        '''\n",
                "        # Load the content of those normalized datasets\n",
                "        text = self._load_texts()\n",
                "\n",
                "        # Tokenizing the entire dataset\n",
                "        data = self._encode(text)\n",
                "\n",
                "        samples: List[Tuple[int, List[int], int]] = []\n",
                "        for i in tqdm(range(1, len(data) - self.block_size), ascii=True, desc='Creating Samples'):\n",
                "            prev_token = data[i-1]                     # previous token of the sequence\n",
                "            input_sequence = data[i:i+self.block_size] # the sequence that will be passed into the model\n",
                "            target_token = data[i+self.block_size]     # next token of the sequence\n",
                "            samples.append((prev_token, input_sequence, target_token))\n",
                "\n",
                "        TransformerShakespeareDataset.logger.info('Samples created succesfully.')\n",
                "\n",
                "        return samples\n",
                "\n",
                "    def encode(self, input_string: str) -> List[int]:\n",
                "        '''\n",
                "        Encode the given string.\n",
                "\n",
                "        Args:\n",
                "            input_string (str): The string to be encoded\n",
                "\n",
                "        Returns:\n",
                "            List[int]: The encoded version of the string\n",
                "        '''\n",
                "        contractions_dict = load_contractions(self.constractions_path)\n",
                "\n",
                "        norm_input = self._normalize(input_string, contractions_dict)\n",
                "\n",
                "        return self._encode(norm_input)\n",
                "    \n",
                "    def decode(self, output_list: List[int]) -> str:\n",
                "        '''\n",
                "        Decode a list of intagers and convert them into string\n",
                "\n",
                "        Args:\n",
                "            output_list (List[int]): The encoded list of intagers\n",
                "\n",
                "        Returns:\n",
                "            str: The decoded string\n",
                "        '''\n",
                "        return self._decode(output_list)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Evaluator Class"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src.utils.evaluation import (\n",
                "    accuracy_fn,\n",
                "    get_precision,\n",
                "    get_recall,\n",
                "    get_specificity,\n",
                "    get_f1_score,\n",
                "    get_perplexity\n",
                ")\n",
                "from src.utils.log import configure_logger\n",
                "from src.utils.models import create_tgt\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "import multiprocessing\n",
                "from multiprocessing.managers import BaseManager\n",
                "from tqdm import tqdm\n",
                "from typing import Callable, Union, List, Dict\n",
                "\n",
                "\n",
                "class TransformerEvaluator:\n",
                "    '''\n",
                "    A class to evaluate a PyTorch model on a test dataset using various metrics.\n",
                "    '''\n",
                "\n",
                "    # Initialize the logger as a class attribute\n",
                "    logger = configure_logger(__name__)\n",
                "\n",
                "    def __init__(self,\n",
                "            model: nn.Module,\n",
                "            test_ds: Dataset,\n",
                "            batch_size: int,\n",
                "            cretirion: nn.Module,\n",
                "            device: torch.device=torch.device('cpu')\n",
                "        ) -> None:\n",
                "        '''\n",
                "        Initializes the TransformerEvaluator with the model, test dataset, loss function, and device.\n",
                "\n",
                "        Args:\n",
                "            model (nn.Module): The neural network model to be evaluated.\n",
                "            test_ds (Dataset): Dataset containing the test data.\n",
                "            batch_size (int): The batch size for creating the DataLoader.\n",
                "            criterion (nn.Module): Loss function used for evaluation.\n",
                "            device (torch.device, optional): Device to run the evaluation on (CPU or GPU). Defaults to CPU.\n",
                "        '''\n",
                "        self.model = model.to(device, non_blocking=True)\n",
                "        self.test_ds = test_ds\n",
                "        self.batch_size = batch_size\n",
                "        self.cretirion = cretirion\n",
                "        self.device = device\n",
                "\n",
                "    def _create_DataLoader(self) -> DataLoader:\n",
                "        '''\n",
                "        Create a DataLoader from the dataset\n",
                "        '''\n",
                "        return DataLoader(\n",
                "            dataset=self.test_ds,\n",
                "            batch_size=self.batch_size,\n",
                "            shuffle=False,\n",
                "            drop_last=True,\n",
                "            num_workers=multiprocessing.cpu_count(),\n",
                "            pin_memory=True\n",
                "        )\n",
                "    \n",
                "    def evaluate(self) -> Dict[str, Union[float, List[List[float]]]]:\n",
                "        '''\n",
                "        Evaluates the model on the test dataset using various metrics.\n",
                "\n",
                "        Returns:\n",
                "            Dict[str, Union[float, List[List[float]]]]: A dictionary containing evaluation metrics.\n",
                "                - \"Loss\": The loss value.\n",
                "                - \"perplexity\": The perplexity value.\n",
                "                - \"accuracy\": Accuracy of the model.\n",
                "                - \"precision\": Precision of the model.\n",
                "                - \"recall\": Recall of the model.\n",
                "                - \"specificity\": Specificity of the model.\n",
                "                - \"f1_score\": F1 score of the model.\n",
                "        '''\n",
                "        def _initialize_results(manager: BaseManager) -> Dict[str, Union[float, List[List[float]]]]:\n",
                "            return manager.dict({\n",
                "                'Loss': 0.0,\n",
                "                'perplexity': 0.0,\n",
                "                'accuracy': 0.0,\n",
                "                'precision': 0.0,\n",
                "                'recall': 0.0,\n",
                "                'specificity': 0.0,\n",
                "                'f1_score': 0.0,\n",
                "            })\n",
                "\n",
                "        def _define_metrics() -> Dict[str, Callable[[torch.Tensor, torch.Tensor], float]]:\n",
                "            return {\n",
                "                'Loss': self.cretirion,\n",
                "                'perplexity': get_perplexity,\n",
                "                'accuracy': accuracy_fn,\n",
                "                'precision': get_precision,\n",
                "                'recall': get_recall,\n",
                "                'specificity': get_specificity,\n",
                "                'f1_score': get_f1_score,\n",
                "            }\n",
                "\n",
                "        def _calculate_and_update(\n",
                "                y_pred: torch.Tensor,\n",
                "                y_true: torch.Tensor,\n",
                "                key: str,\n",
                "                metric: Callable[[torch.Tensor, torch.Tensor], Union[List[float], float]],\n",
                "            ) -> None:\n",
                "            '''\n",
                "            Calculate the specified metric and update the results dictionary.\n",
                "\n",
                "            Args:\n",
                "                y_pred (torch.Tensor): The predicted labels.\n",
                "                y_true (torch.Tensor): The ground truth labels.\n",
                "                key (str): The metric name.\n",
                "                metric (Callable[[torch.Tensor, torch.Tensor], Union[List[float], float]]): The metric function.\n",
                "            '''\n",
                "            nonlocal results\n",
                "            if key == 'Loss':\n",
                "                results[key] = metric(y_pred, y_true.to(torch.long)).item()\n",
                "            else:\n",
                "                results[key] = metric(y_pred, y_true)\n",
                "\n",
                "        manager = multiprocessing.Manager()\n",
                "        results = _initialize_results(manager)\n",
                "        metrics = _define_metrics()\n",
                "\n",
                "        TransformerEvaluator.logger.info('Start Evaluation Process.')\n",
                "\n",
                "        test_dl = self._create_DataLoader()\n",
                "        y_pred = []\n",
                "        y_true = []\n",
                "\n",
                "        self.model.eval()\n",
                "        with torch.inference_mode():\n",
                "            for prev_batch, x_batch, y_batch in tqdm(test_dl, ascii=True, desc='Producing Predictions'):\n",
                "                # Move samples to the same device as the model\n",
                "                x_batch = x_batch.to(self.device, non_blocking=True)\n",
                "                y_batch = y_batch.to(self.device, non_blocking=True)\n",
                "\n",
                "                # Produce the target tensor and generate logits\n",
                "                tgt = create_tgt(prev_batch, x_batch)\n",
                "                y_logits = self.model(x_batch, tgt)\n",
                "\n",
                "                for batch in y_logits:\n",
                "                    y_pred.append(batch.tolist())\n",
                "                y_true.extend(y_batch.tolist())\n",
                "\n",
                "        # Start multiprocessing for metric calculations\n",
                "        processes = []\n",
                "        for metric_name, metric_fn in tqdm(metrics.items(), ascii=True, desc='Calculating Metrics'):\n",
                "            process = multiprocessing.Process(target=_calculate_and_update, args=(torch.tensor(y_pred), torch.tensor(y_true, dtype=torch.float32), metric_name, metric_fn))\n",
                "            processes.append(process)\n",
                "            process.start()\n",
                "\n",
                "        # Ensure all processes have completed\n",
                "        for process in processes:\n",
                "            process.join()\n",
                "\n",
                "        TransformerEvaluator.logger.info(\"Evaluation Process Completed Successfully.\")\n",
                "\n",
                "        return dict(results)\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Trainer Class"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src.utils.save import save_model\n",
                "from src.utils.log import configure_logger\n",
                "from src.utils.training import get_loaders\n",
                "from src.utils.models import create_tgt\n",
                "\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.optim.lr_scheduler import LRScheduler\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "\n",
                "from multiprocessing import cpu_count\n",
                "\n",
                "from tqdm import tqdm\n",
                "from timeit import default_timer as timer\n",
                "from typing import Callable, Tuple, Dict, Union, List\n",
                "\n",
                "\n",
                "class TransformerTrainer:\n",
                "    '''\n",
                "    A class to handle the training of a PyTorch model.\n",
                "    '''\n",
                "\n",
                "    # Initialize the logger as a class attribute\n",
                "    logger = configure_logger(__name__)\n",
                "\n",
                "    def __init__(self,\n",
                "            model: nn.Module,\n",
                "            dataset: Dataset,\n",
                "            batch_size: int,\n",
                "            criterion : Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
                "            eval_fn: Callable[[torch.Tensor, torch.Tensor], float],\n",
                "            opt: torch.optim.Optimizer,\n",
                "            scheduler: Union[LRScheduler, None]=None,\n",
                "            train_prop: float=0.8,\n",
                "            step: int=1,\n",
                "            device: torch.device=torch.device('cpu'),\n",
                "        ) -> None:\n",
                "        '''\n",
                "        Initializes the TransformerTrainer with the model, data loaders, loss function, evaluation function, and optimizer.\n",
                "\n",
                "        Args:\n",
                "            model (nn.Module): The neural network model to be trained.\n",
                "            dataset (Dataset): The dataset the will be train the model.\n",
                "            batch_size (int): The batch_size of the model's DataLoaders.\n",
                "            criterion (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): Loss function.\n",
                "            eval_fn (Callable[[torch.Tensor, torch.Tensor], torch.Tensor]): Evaluation function.\n",
                "            opt (torch.optim.Optimizer): Optimizer for updating the model parameters.\n",
                "            train_prop (int): The propotion of the dataset that will be use to train the model. Default is 80%.\n",
                "            step (int): The step of the cross validational batches.\n",
                "            device (torch.device): The device that the model will be trained on. Default is cpu.\n",
                "        '''\n",
                "        self.model = model.to(device, non_blocking=True)\n",
                "        self.dataset = dataset\n",
                "        self.batch_size = batch_size\n",
                "        self.criterion  = criterion\n",
                "        self.eval_fn = eval_fn\n",
                "        self.opt = opt\n",
                "        self.scheduler = scheduler\n",
                "        self.train_prop = train_prop\n",
                "        self.step = step\n",
                "        self.device = device\n",
                "\n",
                "    def _get_loaders(self) -> Tuple[DataLoader, DataLoader]:\n",
                "        # Get the training and validation Loaders for the epoch\n",
                "        train_ds, valid_ds = random_split(self.dataset, [self.train_prop, 1 - self.train_prop])\n",
                "\n",
                "        train_dl = DataLoader(train_ds, self.batch_size, shuffle=True, num_workers=cpu_count(), pin_memory=True)\n",
                "        valid_dl = DataLoader(valid_ds, self.batch_size, shuffle=False, num_workers=cpu_count(), pin_memory=True)\n",
                "\n",
                "        return train_dl, valid_dl\n",
                "\n",
                "    def _get_loaders_cv(self, offset: Union[List[int], int]) -> Tuple[DataLoader, DataLoader, int]:\n",
                "        train_dl, valid_dl, offset = get_loaders(\n",
                "            dataset=self.dataset,\n",
                "            batch_size=self.batch_size,\n",
                "            train_pro=self.train_prop,\n",
                "            drop_last=True,\n",
                "            offset=offset,\n",
                "            step=self.step\n",
                "        )\n",
                "\n",
                "        return train_dl, valid_dl, offset\n",
                "    \n",
                "    def _process_data_loaders(self, dl: DataLoader) -> Tuple[float, float]:\n",
                "        '''\n",
                "        Process batches from a DataLoader for either training or validation.\n",
                "\n",
                "        This method iterates over batches of data from a given DataLoader (`dl`), computes\n",
                "        the loss and evaluation metrics for each batch, and optionally performs gradient \n",
                "        descent (backpropagation) if the model is in training mode.\n",
                "\n",
                "        Args:\n",
                "            dl (DataLoader): DataLoader containing batches of data.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, float]: A tuple containing the average batch loss and evaluation \n",
                "                score across all batches in the DataLoader.\n",
                "        '''\n",
                "        # Initialize batch loss and accuracy\n",
                "        batch_loss, batch_eval = 0.0, 0.0\n",
                "\n",
                "        phase = 'Training Step' if self.model.training else 'Validation Step'\n",
                "\n",
                "        for prev_batch, x_batch, y_batch in tqdm(dl, ascii=True, desc=f'             {phase}'):\n",
                "            # Moving batches to device\n",
                "            prev_batch = prev_batch.to(self.device, non_blocking=True) # batch containing all the previous tokens of the sequence\n",
                "            x_batch = x_batch.to(self.device, non_blocking=True)       # batch containing the sequence to be passed into the model\n",
                "            y_batch = y_batch.to(self.device, non_blocking=True)       # batch containing the next token of the sequence\n",
                "\n",
                "            # Creating the inputs of the embedding layer of the decoder, which consist of the previous tokens in the sequence\n",
                "            tgt = create_tgt(prev_batch, x_batch)\n",
                "\n",
                "            # Generating predictions (forward pass)\n",
                "            model_logits = self.model(x_batch, tgt)\n",
                "\n",
                "            # Calculate loss\n",
                "            loss = self.criterion(model_logits, y_batch)\n",
                "            batch_loss += loss.item()\n",
                "            batch_eval += self.eval_fn(model_logits, y_batch)\n",
                "\n",
                "            # Backward pass and optimizer step (only for training)\n",
                "            if self.model.training:\n",
                "                self.opt.zero_grad()\n",
                "                loss.backward()\n",
                "                self.opt.step()\n",
                "\n",
                "        batch_loss /= len(dl)\n",
                "        batch_eval /= len(dl)\n",
                "\n",
                "        return batch_loss, batch_eval\n",
                "    \n",
                "    def _training_step(self, train_dl: Union[DataLoader, List[DataLoader]]) -> Tuple[float, float]:\n",
                "        '''\n",
                "        Performs a single training step over the training DataLoader.\n",
                "\n",
                "        Args:\n",
                "            train_dl (DataLoader): The training dataloader that will fit the model.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, float]: The average training loss and evaluation score for the epoch.\n",
                "        '''\n",
                "        self.model.train()\n",
                "\n",
                "        train_loss, train_eval = self._process_data_loaders(train_dl)\n",
                "\n",
                "        self.model.eval()\n",
                "        \n",
                "        return train_loss, train_eval\n",
                "    \n",
                "    def _validation_step(self, valid_dl: Union[DataLoader, List[DataLoader]]) -> Tuple[float, float]:\n",
                "        '''\n",
                "        Performs a single validation step over the validation DataLoader.\n",
                "\n",
                "        Args:\n",
                "            valid_dl (Dataloader): The validation dataloader to evaluate the model.\n",
                "\n",
                "        Returns:\n",
                "            Tuple[float, float]: The average validation loss and evaluation score for the epoch.\n",
                "        '''\n",
                "        self.model.eval()\n",
                "\n",
                "        with torch.inference_mode():\n",
                "            valid_loss, valid_eval = self._process_data_loaders(valid_dl)\n",
                "\n",
                "        return valid_loss, valid_eval\n",
                "\n",
                "    def fit(self,\n",
                "            epochs: int,\n",
                "            save_per: Union[int, None]=None,\n",
                "            save_path: Union[str, None]=None,\n",
                "            save_best: bool=False,\n",
                "            cross_validate: bool=False,\n",
                "        ) -> Dict[str, Union[List[float], str, int]]:\n",
                "        '''\n",
                "        Trains the model for a specified number of epochs and optionally saves checkpoints.\n",
                "\n",
                "        Args:\n",
                "            epochs (int): The number of epochs to train the model for.\n",
                "            save_per (Union[int, None], optional): Frequency (in epochs) to save model checkpoints. Defaults to None.\n",
                "            save_path (Union[str, None], optional): The path that the checkpoints will be saved on. Defaults to None.\n",
                "            cross_validate (bool, optional): Whether to use cross-validation. Default is False.\n",
                "\n",
                "        Returns:\n",
                "            Dict[str, Union[List[float], str, int]]: A dictionary containing training statistics and metadata.\n",
                "                It includes:\n",
                "                - 'train_loss': List of training losses for each epoch.\n",
                "                - 'train_eval': List of training evaluation scores for each epoch.\n",
                "                - 'valid_loss': List of validation losses for each epoch.\n",
                "                - 'valid_eval': List of validation evaluation scores for each epoch.\n",
                "                - 'model_name': Name of the model class.\n",
                "                - 'loss_fn': Name of the loss function class.\n",
                "                - 'eval_fn': Name of the evaluation function.\n",
                "                - 'optimizer': Name of the optimizer class.\n",
                "                - 'device': Type of device the model is on.\n",
                "                - 'epochs': Total number of epochs trained.\n",
                "                - 'total_time': Total time taken for training and evaluation.\n",
                "                - 'save_path': Saved path of the checkpoints.\n",
                "                - 'cross_validate': Wheather cross-validation is being used.\n",
                "        '''\n",
                "        start_time = timer()\n",
                "        train_losses, train_evals = [], []\n",
                "        valid_losses, valid_evals = [], []\n",
                "        best_valid_loss = float('inf')\n",
                "\n",
                "        # the variable of cross validation\n",
                "        offset = 0\n",
                "\n",
                "        TransformerTrainer.logger.info('Start Training Process.')\n",
                "\n",
                "        # Get the loaders if the user doesn't want to use cross validation\n",
                "        if not cross_validate:\n",
                "            TransformerTrainer.logger.info('Creating training and validation DataLoaders.')\n",
                "            train_dl, valid_dl = self._get_loaders()\n",
                "            TransformerTrainer.logger.info('Dataloaders created succesfully.')\n",
                "\n",
                "        for epoch in range(1, epochs + 1):\n",
                "            TransformerTrainer.logger.info(f'-> Epoch: {epoch}/{epochs}')\n",
                "\n",
                "            # Get the loaders if the user want to use cross validation\n",
                "            if cross_validate:\n",
                "                TransformerTrainer.logger.info('    Creating training and validation DataLoaders (cross-validation step)')\n",
                "                train_dl, valid_dl, offset = self._get_loaders_cv(offset)\n",
                "                TransformerTrainer.logger.info('    Dataloaders created succesfully.')\n",
                "\n",
                "            # Training and Evaluating the Model\n",
                "            train_loss, train_eval = self._training_step(train_dl)\n",
                "            valid_loss, valid_eval = self._validation_step(valid_dl)\n",
                "\n",
                "            # Log the results\n",
                "            TransformerTrainer.logger.info('')\n",
                "            TransformerTrainer.logger.info(f'    Results (lr={self.opt.param_groups[0][\"lr\"]:.6f}):')\n",
                "            TransformerTrainer.logger.info(f'    Train Loss:       {train_loss:.4f}')\n",
                "            TransformerTrainer.logger.info(f'    Train Eval Score: {train_eval:.4f}')\n",
                "            TransformerTrainer.logger.info(f'    Valid Loss:       {valid_loss:.4f}')\n",
                "            TransformerTrainer.logger.info(f'    Valid Eval Score: {valid_eval:.4f}')\n",
                "\n",
                "            train_losses.append(train_loss)\n",
                "            train_evals.append(train_eval)\n",
                "            valid_losses.append(valid_loss)\n",
                "            valid_evals.append(valid_eval)\n",
                "\n",
                "            # Step the scheduler\n",
                "            if self.scheduler is not None:\n",
                "                self.scheduler.step()\n",
                "                TransformerTrainer.logger.info('')\n",
                "                TransformerTrainer.logger.info(f'    Scheduling step executed succesfully (new lr={self.opt.param_groups[0][\"lr\"]:.6f})')\n",
                "\n",
                "            # Save the best generator model based on loss\n",
                "            if save_best and valid_loss < best_valid_loss:\n",
                "                best_valid_loss = valid_loss\n",
                "                save_model(self.model, f'{save_path}/{self.model.__class__.__name__}_best.pth')\n",
                "\n",
                "            # Saving the model\n",
                "            if save_per and save_path and (epoch % save_per == 0):\n",
                "                save_model(self.model, f'{save_path}/{self.model.__class__.__name__}_checkpoint_{epoch}.pth')\n",
                "\n",
                "            TransformerTrainer.logger.info(('-' * 100))\n",
                "\n",
                "        # After training, clear the CUDA cache\n",
                "        if self.device.type == 'cuda':\n",
                "            torch.cuda.empty_cache()\n",
                "\n",
                "        TransformerTrainer.logger.info('Training Process Completed Successfully.')\n",
                "\n",
                "        return {\n",
                "            'train_loss': train_losses,\n",
                "            'train_eval': train_evals,\n",
                "            'valid_loss': valid_losses,\n",
                "            'valid_eval': valid_evals,\n",
                "            'model_name': self.model.__class__.__name__,\n",
                "            'loss_fn': self.criterion.__class__.__name__,\n",
                "            'eval_fn': self.eval_fn.__name__,\n",
                "            'optimizer': self.opt.__class__.__name__,\n",
                "            'scheduler': self.scheduler.__class__.__name__,\n",
                "            'device': self.device.type,\n",
                "            'epochs': epochs,\n",
                "            'total_time': timer() - start_time,\n",
                "            'save_path': save_path,\n",
                "            'cross_validate': cross_validate\n",
                "        }\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Models"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src.utils.training import PositionalEncoding\n",
                "from src.utils.models import temperature_sampling, create_tgt\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "from typing import List\n",
                "\n",
                "\n",
                "class LSTMCharModel(nn.Module):\n",
                "    def __init__(self,\n",
                "            block_size: int,\n",
                "            vocab_size: int,\n",
                "            embedding_dim: int,\n",
                "            hidden_dim: int,\n",
                "            num_layers: int,\n",
                "            dropout: int\n",
                "        ) -> None:\n",
                "        super().__init__()\n",
                "\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
                "\n",
                "        self.lstm = nn.LSTM(\n",
                "            input_size=embedding_dim,\n",
                "            hidden_size=hidden_dim,\n",
                "            num_layers=num_layers,\n",
                "            dropout=dropout,\n",
                "            batch_first=True,\n",
                "            bidirectional=True\n",
                "        )\n",
                "\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(2*hidden_dim*block_size, vocab_size)\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        x = self.embedding(x)\n",
                "        x, _ = self.lstm(x)\n",
                "        x = self.dropout(x)\n",
                "\n",
                "        return self.fc(x)\n",
                "\n",
                "\n",
                "class TransformerCharModel(nn.Module):\n",
                "    def __init__(self,\n",
                "            vocab_size: int,\n",
                "            block_size: int,\n",
                "            embedding_dim: int,\n",
                "            n_head: int,\n",
                "            n_encoders: int,\n",
                "            n_decoders: int,\n",
                "            dim_feedforward: int,\n",
                "            dropout: float,\n",
                "            device: torch.device\n",
                "        ) -> None:\n",
                "        super().__init__()\n",
                "        self.block_size = block_size\n",
                "        self.embedding_dim = embedding_dim\n",
                "        self.vocab_size = vocab_size\n",
                "        self.device = device\n",
                "\n",
                "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
                "        self.positional_embeddings = PositionalEncoding(block_size, embedding_dim)\n",
                "\n",
                "        encoder_layer = nn.TransformerEncoderLayer(\n",
                "            d_model=embedding_dim,\n",
                "            nhead=n_head,\n",
                "            dim_feedforward=dim_feedforward,\n",
                "            dropout=dropout,\n",
                "            norm_first=True,\n",
                "            activation='gelu',\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_encoders, enable_nested_tensor=False)\n",
                "\n",
                "        decoder_layer = nn.TransformerDecoderLayer(\n",
                "            d_model=embedding_dim,\n",
                "            nhead=n_head,\n",
                "            dim_feedforward=dim_feedforward,\n",
                "            dropout=dropout,\n",
                "            norm_first=True,\n",
                "            activation='gelu',\n",
                "            batch_first=True\n",
                "        )\n",
                "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_decoders)\n",
                "\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.norm = nn.LayerNorm(embedding_dim)\n",
                "\n",
                "        self.classification_head = nn.Sequential(\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(in_features=embedding_dim*block_size, out_features=128),\n",
                "            nn.GELU(),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.Linear(in_features=128, out_features=vocab_size)\n",
                "        )\n",
                "\n",
                "        self._init_weights()\n",
                "\n",
                "    def _init_weights(self):\n",
                "        for p in self.parameters():\n",
                "            if p.dim() > 1:\n",
                "                nn.init.xavier_uniform_(p)\n",
                "\n",
                "    def _generate_square_subsequent_mask(self, size: int) -> torch.Tensor:\n",
                "        '''\n",
                "        Create a mask for the target sequence to prevent the model from looking ahead.\n",
                "        '''\n",
                "        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
                "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
                "        return mask\n",
                "    \n",
                "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
                "        '''\n",
                "        - The `src` provides contextual information to the encoder, which the decoder uses\n",
                "            along with the partially generated `tgt` sequence to produce the next token.\n",
                "        - Teacher Forcing: During training, the tgt sequence is used to guide the model in\n",
                "            generating the correct sequence. This technique, known as teacher forcing, helps\n",
                "            the model learn to generate sequences efficiently.\n",
                "        '''\n",
                "        # Get the embeddings for the input tokens\n",
                "        src = self.embeddings(src) + self.positional_embeddings(src)\n",
                "        tgt = self.embeddings(tgt) + self.positional_embeddings(tgt)\n",
                "\n",
                "        # Normalization and dropout\n",
                "        src = self.norm(self.dropout(src))\n",
                "        tgt = self.norm(self.dropout(tgt))\n",
                "\n",
                "        # Prevents the model from attending to future tokens in the target sequence (look-ahead mask).\n",
                "        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(self.device)\n",
                "\n",
                "        # Transformer encoder and decoder\n",
                "        memory = self.transformer_encoder(src)\n",
                "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask)\n",
                "\n",
                "        # Normalization, dropout, and classification head\n",
                "        output = self.norm(self.dropout(output))\n",
                "        return self.classification_head(output)\n",
                "\n",
                "    def generate(self, first_token: torch.Tensor, initial_tokens: torch.Tensor, max_length: int, temperature: float) -> List[int]:\n",
                "        '''\n",
                "        Generate a sequence of tokens using the model, starting from an initial set of tokens and generating up to a specified maximum length.\n",
                "\n",
                "        Args:\n",
                "            first_token (torch.Tensor): A tensor containing the first token to start the sequence generation.\n",
                "            initial_tokens (torch.Tensor): A tensor containing the initial sequence of tokens.\n",
                "            max_length (int): The maximum length of the sequence to be generated.\n",
                "            temperature (float): A temperature parameter used for controlling the randomness of predictions during sampling.\n",
                "\n",
                "        Returns:\n",
                "            List[int]: A list of generated token IDs.\n",
                "        '''\n",
                "        # generated_tokens = [first_token.item()] + initial_tokens.tolist()\n",
                "        generated_tokens = first_token.tolist()\n",
                "        generated_tokens.extend(initial_tokens.squeeze(dim=0).tolist())\n",
                "\n",
                "        initial_idx = 0\n",
                "        for _ in range(max_length):\n",
                "            # Get the current input sequence\n",
                "            tokens_cond = initial_tokens[:, -self.block_size:]\n",
                "            # tgt = create_tgt(first_token, initial_tokens)\n",
                "            tgt = create_tgt(first_token, tokens_cond)\n",
                "\n",
                "            # Forward pass through the model\n",
                "            model_logits = self(tokens_cond, tgt)\n",
                "            next_token = temperature_sampling(model_logits.squeeze(dim=0), temperature)\n",
                "\n",
                "            # Append the generated token to the sequence\n",
                "            generated_tokens.append(next_token)\n",
                "\n",
                "            # Resize the next token to concatenate it to the `initial_tokens`\n",
                "            next_token = torch.tensor([[next_token]]).to(self.device, non_blocking=True)\n",
                "            \n",
                "            # Update the sequence for the next prediction\n",
                "            initial_tokens = torch.cat((initial_tokens, next_token), dim=1)\n",
                "            first_token[0] = initial_tokens[:, initial_idx]\n",
                "            initial_idx += 1\n",
                "\n",
                "        return generated_tokens\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Train"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src import config\n",
                "from src.dataset import TransformerShakespeareDataset\n",
                "from src.models import TransformerCharModel\n",
                "from src.training import TransformerTrainer\n",
                "from src.utils import (\n",
                "    configure_logger,\n",
                "    accuracy_fn,\n",
                "    plot_losses,\n",
                "    get_device\n",
                ")\n",
                "\n",
                "from torch import nn, optim\n",
                "from torch.optim.lr_scheduler import ExponentialLR\n",
                "\n",
                "import os\n",
                "\n",
                "\n",
                "# Get the logger for this module\n",
                "logger = configure_logger(__name__)\n",
                "\n",
                "# Get default device\n",
                "device = get_device()\n",
                "\n",
                "\n",
                "def main() -> None:\n",
                "    '''Main function to train the deep learning model.'''\n",
                "\n",
                "    # Initialize the dataset object\n",
                "    dataset = TransformerShakespeareDataset(\n",
                "        dataset_path=config.TRAIN_DATASETS_PATH,\n",
                "        norm_dataset_path=config.NORMALIZE_TRAIN_DATA_PATH,\n",
                "        constractions_path=config.CONTRACTIONS_PATH,\n",
                "        block_size=config.BLOCK_SIZE,\n",
                "        to_tensors=True,\n",
                "        write_norm=False,\n",
                "        device=device\n",
                "    )\n",
                "\n",
                "    # Saving the vocab size because we'll use it for evaluating the model\n",
                "    vocab_size = dataset.vocab_size\n",
                "\n",
                "    # Instanciate the Model\n",
                "    model = TransformerCharModel(block_size=config.BLOCK_SIZE, vocab_size=vocab_size, device=device, **config.TRANSFORMER_CONFIGS).to(device, non_blocking=True)\n",
                "\n",
                "    logger.info(f'The model is created and placed on the device: {device.type}')\n",
                "\n",
                "    loss_fn = nn.CrossEntropyLoss()\n",
                "\n",
                "    opt = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
                "    scheduler = ExponentialLR(opt, gamma=config.GAMMA)\n",
                "\n",
                "    # Instanciate the Trainer\n",
                "    trainer = TransformerTrainer(\n",
                "        model = model,\n",
                "        dataset = dataset,\n",
                "        batch_size = config.BATCH_SIZE,\n",
                "        criterion = loss_fn,\n",
                "        eval_fn = accuracy_fn,\n",
                "        opt = opt,\n",
                "        scheduler=scheduler,\n",
                "        device = device,\n",
                "    )\n",
                "\n",
                "    logger.info('The trainer is created.')\n",
                "\n",
                "    # Train the model\n",
                "    train_res = trainer.fit(\n",
                "        epochs = config.EPOCHS,\n",
                "        save_per = config.EPOCHS,\n",
                "        save_path = config.MODELS_PATH,\n",
                "        cross_validate = False,\n",
                "        save_best=True\n",
                "    )\n",
                "\n",
                "    logger.info(f'Training Results: {train_res}')\n",
                "\n",
                "    # Plot the losses and save them\n",
                "    plot_losses(train_res['train_loss'], train_res['valid_loss'], save_path=os.path.join(config.PLOTS_PATH, 'losses.png'))\n",
                "\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    main()\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Evaluate"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src import config\n",
                "from src.dataset import TransformerShakespeareDataset\n",
                "from src.models import TransformerCharModel\n",
                "from src.evaluator import TransformerEvaluator\n",
                "from src.utils.save import load_model\n",
                "from src.utils.log import configure_logger\n",
                "from src.utils import get_device\n",
                "\n",
                "from torch import nn\n",
                "\n",
                "import os\n",
                "from typing import Dict\n",
                "\n",
                "\n",
                "# Get the logger for this module\n",
                "logger = configure_logger(__name__)\n",
                "\n",
                "# Get default device\n",
                "device = get_device()\n",
                "\n",
                "# The vocab_size from training the model\n",
                "VOCAB_SIZE = 72\n",
                "\n",
                "\n",
                "def evaluate(model: nn.Module, dataset: TransformerShakespeareDataset, loss_fn: nn.Module, file: str) -> Dict[str, int]:\n",
                "    '''\n",
                "    Evaluate the model on the given dataset.\n",
                "\n",
                "    Args:\n",
                "        model (nn.Module): The model to evaluate.\n",
                "        dataset (Dataset): The dataset to evaluate on.\n",
                "        loss_fn (nn.Module): The loss function to use.\n",
                "        file (str): Filename to save the evaluation results.\n",
                "\n",
                "    Returns:\n",
                "        Dict[str, float]: Evaluation results.\n",
                "    '''\n",
                "    evaluator = TransformerEvaluator(\n",
                "        model = model,\n",
                "        test_ds = dataset,\n",
                "        cretirion = loss_fn,\n",
                "        batch_size=config.BATCH_SIZE,\n",
                "        device = device\n",
                "    )\n",
                "    logger.info('The evaluator is created.')\n",
                "\n",
                "    eval_res = evaluator.evaluate()\n",
                "\n",
                "    os.makedirs(config.LOGS_PATH, exist_ok=True)\n",
                "\n",
                "    with open(os.path.join(config.LOGS_PATH, file), 'w') as f:\n",
                "        f.write(str(eval_res))\n",
                "\n",
                "    return eval_res\n",
                "\n",
                "\n",
                "def main() -> None:\n",
                "    '''Main function to evaluate the deep learning model.'''\n",
                "\n",
                "    # Initialize the dataset object\n",
                "    dataset = TransformerShakespeareDataset(\n",
                "        dataset_path=config.TEST_DATASETS_PATH,\n",
                "        norm_dataset_path=config.NORMALIZE_TEST_DATA_PATH,\n",
                "        constractions_path=config.CONTRACTIONS_PATH,\n",
                "        block_size=config.BLOCK_SIZE,\n",
                "        to_tensors=True,\n",
                "        device=device\n",
                "    )\n",
                "\n",
                "    # Instanciate the Model and load the pre-trained\n",
                "    model_name = 'TransformerCharModel_checkpoint_20.pth'\n",
                "    model = load_model(\n",
                "        model_class=TransformerCharModel,\n",
                "        model_path=os.path.join(config.MODELS_PATH, model_name),\n",
                "        model_device=True,\n",
                "        block_size=config.BLOCK_SIZE,\n",
                "        vocab_size=VOCAB_SIZE,\n",
                "        device=device,\n",
                "        **config.TRANSFORMER_CONFIGS\n",
                "    ).to(device, non_blocking=True)\n",
                "\n",
                "    logger.info(f'The model is loaded and moved to device: {device.type}')\n",
                "\n",
                "    loss_fn = nn.CrossEntropyLoss()\n",
                "    \n",
                "    # Evaluate the model\n",
                "    results = evaluate(model, dataset, loss_fn, file='transformer_evaluate.txt')\n",
                "\n",
                "    logger.info(f'Results: {results}')\n",
                "\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    main()\n"
            ],
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Generate"
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from src import config\n",
                "from src.dataset import TransformerShakespeareDataset\n",
                "from src.models import TransformerCharModel\n",
                "from src.utils import configure_logger, get_device, load_model\n",
                "\n",
                "import torch\n",
                "\n",
                "import os\n",
                "from typing import Tuple\n",
                "\n",
                "\n",
                "# Get the logger for this module\n",
                "logger = configure_logger(__name__)\n",
                "\n",
                "# Get default device\n",
                "device = get_device()\n",
                "\n",
                "\n",
                "def process_input(dataset: TransformerShakespeareDataset, message: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "    '''\n",
                "    Encode the given input and get it ready to pass it to the model\n",
                "\n",
                "    Args:\n",
                "        dataset (TransformerShakespeareDataset): The dataset object in which we get the encode method\n",
                "        message (str): The message to be encoded and converted to tensor\n",
                "\n",
                "    Returns:\n",
                "        Tuple[torch.Tensor, torch.Tensor]: The input to the model (initial seed for predictions)\n",
                "    '''\n",
                "    encoded_message = dataset.encode(message)[:config.BLOCK_SIZE + 1]\n",
                "    prev_token = torch.tensor(encoded_message[0], dtype=torch.int32).unsqueeze(dim=0).to(device)\n",
                "    model_input = torch.tensor(encoded_message[1:], dtype=torch.int32).unsqueeze(dim=0).to(device)\n",
                "    \n",
                "    return prev_token, model_input\n",
                "\n",
                "\n",
                "def main() -> None:\n",
                "    # Initialize the dataset object to get encoder-decoder and vocab_size\n",
                "    dataset = TransformerShakespeareDataset(\n",
                "        dataset_path=config.TRAIN_DATASETS_PATH,\n",
                "        norm_dataset_path=config.NORMALIZE_TRAIN_DATA_PATH,\n",
                "        constractions_path=config.CONTRACTIONS_PATH,\n",
                "        block_size=config.BLOCK_SIZE,\n",
                "        to_tensors=True,\n",
                "        write_norm=False,\n",
                "        device=device\n",
                "    )\n",
                "\n",
                "    os.makedirs(config.GENERATE_PATH, exist_ok=True)\n",
                "\n",
                "    # Create the initial seed for the prediction\n",
                "    message = '''Peter, the Great Emperor\n",
                "\n",
                "**** ACT I ****\n",
                "**** SCENE I. France. In the battlefield. ****\n",
                "     Enter Peter with his sword.\n",
                "Peter\n",
                " '''\n",
                "    first_token, initial_tokens = process_input(dataset, message)\n",
                "    logger.info(f'Input has been encoded and moved to device: {device.type}')\n",
                "\n",
                "    # Load the pre-trained model\n",
                "    model_name = 'checkpoints/TransformerCharModel_checkpoint_20.pth'\n",
                "    model = load_model(\n",
                "        model_class=TransformerCharModel,\n",
                "        model_path=model_name,\n",
                "        device=device,\n",
                "        model_device=True,\n",
                "        block_size=config.BLOCK_SIZE,\n",
                "        vocab_size=dataset.vocab_size,\n",
                "        **config.TRANSFORMER_CONFIGS\n",
                "    ).to(device)\n",
                "\n",
                "    logger.info(f'The model is loaded and moved to device: {device.type}')\n",
                "\n",
                "    # Generate predictions\n",
                "    tokens = model.generate(first_token, initial_tokens, max_length=1000, temperature=1.0)\n",
                "    decoded_preds = dataset.decode(tokens)\n",
                "    logger.info(f'Predictions have been made and decoded succesfully, saving them into {os.path.join(config.GENERATE_PATH, 'transformer_generation.txt')}')\n",
                "\n",
                "    # Saving predictions\n",
                "    with open(os.path.join(config.GENERATE_PATH, 'generation.txt'), 'w') as f:\n",
                "        f.write(decoded_preds)\n",
                "\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    main()\n"
            ],
            "outputs": [],
            "execution_count": null
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}